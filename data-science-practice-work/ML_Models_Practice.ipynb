{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab22c07b",
   "metadata": {},
   "source": [
    "## Transformers:\n",
    "\n",
    "#### Defination: A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP) and computer vision (CV).\n",
    "\n",
    "###### Source: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba8b731",
   "metadata": {},
   "source": [
    "## Bidirectional Encoder Representations from Transformers (BERT) Model: NLP\n",
    "\n",
    "#### *Defination*: It is based on Transformers, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection. \n",
    "\n",
    "##### It was trained by Google on 2500M words in WikiPedia and trained the model on two approaches: 1. Mass Training model, 2. Next Statement Prediction. Google Search is powered by BERT model.\n",
    "\n",
    "###### Source: https://www.youtube.com/watch?v=7kLi8u2dJz0&ab_channel=codebasics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540ace9",
   "metadata": {},
   "source": [
    "#### Code Courtsey: https://www.youtube.com/watch?v=7kLi8u2dJz0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28873a3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing the desired/required tensorflow libraries:\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fe13392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the useful models from tenserflow_hub repo:\n",
    "\n",
    "pre_processor_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5878dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wraps a SavedModel (or a legacy TF1 Hub format) as a Keras Layer:\n",
    "\n",
    "bert_pp_model = hub.KerasLayer(pre_processor_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3fe6b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_type_ids', 'input_word_ids', 'input_mask'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we have a test text which we are storing in test_text variable and using that to pass \n",
    "# and test it in the tensorflow model and return the keys of the dict after the model pre_processes it:\n",
    "\n",
    "text_test = ['nice_movie_indeed', 'I love Python Programming']\n",
    "text_pre_processed = bert_pp_model(text_test)\n",
    "text_pre_processed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ae5a465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[  101,  3835,  1035,  3185,  1035,  5262,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  101,  1045,  2293, 18750,  4730,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are we doing:\n",
    "\n",
    "# text_pre_processed['input_mask']\n",
    "# text_pre_processed['input_type_ids']\n",
    "text_pre_processed['input_word_ids']\n",
    "\n",
    "# The way BERT works is, it will always put a special token ahead of it called CLS \n",
    "# and one separator token at the end called SEP.\n",
    "# Maxumum length of a sentence: 128\n",
    "# CLS nice_movie_indeed SEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a38b9c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder_outputs', 'pooled_output', 'sequence_output', 'default'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we are creating another layer after pre-processing using encor_url, which gives us three output:\n",
    "\n",
    "bert_model = hub.KerasLayer(encoder_url)\n",
    "bert_results = bert_model(text_pre_processed)\n",
    "bert_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e661ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.8119873 , -0.26844186, -0.06845339, ..., -0.13436879,\n",
       "        -0.52355427,  0.8427556 ],\n",
       "       [-0.91712296, -0.4793517 , -0.7865697 , ..., -0.6175173 ,\n",
       "        -0.7102685 ,  0.92184293]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are checking the embedding. The vector size is 768 for 'nice_movie_indeed'. Similarly for other \n",
    "\n",
    "bert_results['pooled_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e6fe919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
       "array([[[-0.04719081,  0.11078794,  0.01651536, ..., -0.17195085,\n",
       "          0.19408041,  0.09896731],\n",
       "        [ 0.3141219 , -0.4812023 ,  0.7276414 , ..., -0.04402124,\n",
       "          0.4528351 , -0.23027787],\n",
       "        [ 0.553434  ,  0.2630476 ,  1.1952829 , ..., -0.48282036,\n",
       "         -0.5119686 , -0.296567  ],\n",
       "        ...,\n",
       "        [ 0.15611318, -0.03490545,  0.63516194, ..., -0.06585784,\n",
       "          0.02830975,  0.07551166],\n",
       "        [-0.04971653, -0.02078105,  0.64803123, ..., -0.10641124,\n",
       "          0.01159183,  0.1212726 ],\n",
       "        [ 0.14216903, -0.02574774,  0.63996345, ..., -0.02877554,\n",
       "          0.04031472,  0.00215075]],\n",
       "\n",
       "       [[-0.0790059 ,  0.3633513 , -0.21101557, ..., -0.1718373 ,\n",
       "          0.16299753,  0.6724265 ],\n",
       "        [ 0.27883515,  0.43716335, -0.3576473 , ..., -0.04463643,\n",
       "          0.38315186,  0.5887984 ],\n",
       "        [ 1.2037671 ,  1.0727018 ,  0.4840877 , ...,  0.24921034,\n",
       "          0.40730911,  0.4048181 ],\n",
       "        ...,\n",
       "        [ 0.08630013,  0.1935384 ,  0.47540036, ...,  0.18880169,\n",
       "         -0.06474102,  0.31318596],\n",
       "        [ 0.15887041,  0.2857266 ,  0.3734078 , ...,  0.09309134,\n",
       "         -0.04969549,  0.38761112],\n",
       "        [-0.08079888, -0.09572833,  0.26809767, ...,  0.13979661,\n",
       "         -0.06315826,  0.2728833 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results['sequence_output']\n",
    "\n",
    "# Since it's contextualized encoding so padding also has values:\n",
    "# nice movie indeed 0 0 0 0 0 <-- 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06982f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we are checking the length of the encoder output, each layer of the 12 layers has 768 size embedding vector:\n",
    "len(bert_results['encoder_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ce861f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
       "array([[[ 0.10655081,  0.0246375 ,  0.04212973, ...,  0.062705  ,\n",
       "          0.04764155, -0.09229398],\n",
       "        [ 1.0485204 ,  1.0608004 ,  1.3805104 , ...,  0.15818965,\n",
       "         -0.2914644 , -0.4520943 ],\n",
       "        [ 0.33819932, -0.01024941,  0.8076432 , ...,  0.19576207,\n",
       "          0.346847  , -1.2290275 ],\n",
       "        ...,\n",
       "        [-0.03990605, -0.26347458,  0.7489512 , ...,  0.35646957,\n",
       "         -0.31062606,  0.08526935],\n",
       "        [-0.12449503, -0.29239565,  0.61743605, ...,  0.38330048,\n",
       "         -0.22637336, -0.01101273],\n",
       "        [-0.00385755, -0.18815611,  0.6656786 , ...,  0.7007718 ,\n",
       "         -0.5560532 , -0.11448625]],\n",
       "\n",
       "       [[ 0.1890359 ,  0.02752548, -0.0651374 , ..., -0.00620213,\n",
       "          0.15053894,  0.03165445],\n",
       "        [ 0.5916149 ,  0.7589137 , -0.07240661, ...,  0.6190394 ,\n",
       "          0.8292891 ,  0.16161954],\n",
       "        [ 1.4460827 ,  0.44602644,  0.4099025 , ...,  0.48255914,\n",
       "          0.62691146,  0.13463417],\n",
       "        ...,\n",
       "        [ 0.15147898, -0.21573842,  0.70329076, ..., -0.12537211,\n",
       "         -0.13787258,  0.2772205 ],\n",
       "        [ 0.051438  , -0.24052706,  0.5356913 , ..., -0.07915058,\n",
       "         -0.03307928,  0.1738092 ],\n",
       "        [ 0.20934707, -0.15645266,  0.60395443, ...,  0.3290352 ,\n",
       "         -0.35827187,  0.08100392]]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we are checking the encoder output of each of the 12 layer:\n",
    "\n",
    "bert_results['encoder_outputs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ad3d0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128, 768), dtype=bool, numpy=\n",
       "array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]]])>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last encoder output is same as the sequence output:\n",
    "\n",
    "bert_results['encoder_outputs'][-1] == bert_results['sequence_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644140b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc855ce8",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) Model: NLP\n",
    "\n",
    "#### Defination: NER is primarily used to extract different yet specific named entities in the text fed into this model. Example of entities can be person, product, company, etc from that particular text data.\n",
    "\n",
    "#### NER can be used in the following:\n",
    "\n",
    "#### 1. Search/Text based recommendation such as News articles, Google search, Customer Support, etc\n",
    "#### 2. Movie recommendation such as Hotstar, Netflix, etc\n",
    "\n",
    "#### Here, we will be using a custom_spacy model(pre-tranined model) to understand the coding aspects:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6bdf59",
   "metadata": {},
   "source": [
    "#### Code Courtsey: https://www.youtube.com/watch?v=2XUhKpH0p4M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86df2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries and the model:\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0db40d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will now see NLP pipeline-names:\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec5bc54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc | ORG | Companies, agencies, institutions, etc.\n",
      "Twitter, Inc | ORG | Companies, agencies, institutions, etc.\n",
      "$45 billion | MONEY | Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# Here, we perform NER analysis of the text input that we have using custom spacy model:\n",
    "\n",
    "doc = nlp(\"Tesla Inc is going to acquire Twitter, Inc for $45 billion\")\n",
    "\n",
    "# So, doc.ents contains all the entities in the above statement:\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7aaec895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twitter, Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here, lets' do the vizualized beautified labelling using the NER spacy's, displacy module:\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")\n",
    "# displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cdc3e3",
   "metadata": {},
   "source": [
    "#### So, our finding is that even the NER mechanism associared with Custom Spacy model or Spacy model is not accurate/perfect, it does have some issues. It is using some specific rules and if that fails, we it failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd07ab90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see all the entities that spacy supports:\n",
    "\n",
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fb73b",
   "metadata": {},
   "source": [
    "##### List of entities are also documented on this page: https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52a3b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Bloomberg | PERSON | People, including fictional\n",
      "Bloomberg L.P. | ORG | Companies, agencies, institutions, etc.\n",
      "1982 | DATE | Absolute or relative dates or periods\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Michael Bloomberg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " founded \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bloomberg L.P.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1982\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we try another example of NER using spacy:\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\"Michael Bloomberg founded Bloomberg L.P. in 1982\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))      \n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdcd545",
   "metadata": {},
   "source": [
    "#### For the above also, it made a mistake in identifying Bloomberg the company. Let's try hugging face for this now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08ebdc",
   "metadata": {},
   "source": [
    "https://huggingface.co/dslim/bert-base-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80429517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter | ORG | Companies, agencies, institutions, etc.\n",
      "$45 billion | MONEY | Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# Let's discuss what's a Span: \n",
    "\n",
    "doc = nlp(\"Tesla is going to acquire Twitter for $45 billion\")\n",
    "\n",
    "# So, doc.ents contains all the entities in the above statement:\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f4f8a216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "print(type(doc[0]))\n",
    "print(type(doc[2:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e24b52e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are checking our knowledge by testing the Span module under Spacy:\n",
    "\n",
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(doc, 0, 1, label='ORG')\n",
    "s2 = Span(doc, 5, 6, label='ORG')\n",
    "\n",
    "doc.set_ents([s1, s2], default='unmodified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26cce68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla | ORG | Companies, agencies, institutions, etc.\n",
      "Twitter | ORG | Companies, agencies, institutions, etc.\n",
      "$45 billion | MONEY | Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b27ca3",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation Model: ML\n",
    "\n",
    "#### Defination: When we have a large data-set which needs to be classified as Spam vs Ham(Not Spam), we need to perform train, test/we need to split the data into train and test for model evaluation.\n",
    "\n",
    "#### Let's take an example, let's say we have a data-set of 100 df's and we want to train it as 70-30(train-test), then only data similar to that ratio will be getting calssified but whenever the data sees new data it will not be able to adapt and perform as per the new data. So, to make it more robust and efficient, we need to perform K-Fold Cross Validation where we basically spilt the data into 80-20(trian-test) ratio and for each fold we keep changing the testing data-set w.r.t. training data-set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675a2b2",
   "metadata": {},
   "source": [
    "#### Code Courtsey: https://www.youtube.com/watch?v=gJo0uNL-5Qw&t=1267s&ab_channel=codebasics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06c2fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required ML libraries:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9319d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and return the digits dataset(classification):\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7078df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import model selection for spliting our data into train_test_split:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6ab2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "475634e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "lr = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear',multi_class='ovr'))\n",
    "# lr = LogisticRegression(solver='liblinear',multi_class='ovr')\n",
    "# apply scaling on training data\n",
    "lr.fit(X_train, y_train)\n",
    "# apply scaling on testing data, without leaking training data.\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b62641c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3685185185185185"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6b4cff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=40)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4a309",
   "metadata": {},
   "source": [
    "##### Here, we see that the scores from Logistic Regression, Random Forest are giving us reliable and comparable values while SVM is giving us relatively lower scores. So, we can conclude from here that we need a much more reliable model for getting scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbacad5",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "831a760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6 7 8] [0 1 2]\n",
      "[0 1 2 6 7 8] [3 4 5]\n",
      "[0 1 2 3 4 5] [6 7 8]\n"
     ]
    }
   ],
   "source": [
    "# Basic example\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "kf\n",
    "\n",
    "KFold(n_splits=3, random_state=None, shuffle=False)\n",
    "for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):\n",
    "    print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ac8450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our use case we define a method(we can use KFold for our digits example):\n",
    "\n",
    "def get_score(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a300401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the steps performed while performing/checking model validation in K-Fold validation, \n",
    "# which we will perform using Stratified K-fold valiation(This is for understanding purposes, we can use cross_val_score):\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=3)\n",
    "\n",
    "scores_logistic = []\n",
    "scores_svm = []\n",
    "scores_rf = []\n",
    "\n",
    "for train_index, test_index in folds.split(digits.data,digits.target):\n",
    "    X_train, X_test, y_train, y_test = digits.data[train_index], digits.data[test_index], \\\n",
    "                                       digits.target[train_index], digits.target[test_index]\n",
    "    scores_logistic.append(get_score(lr, X_train, X_test, y_train, y_test))\n",
    "    scores_svm.append(get_score(SVC(gamma='auto'), X_train, X_test, y_train, y_test))\n",
    "    scores_rf.append(get_score(RandomForestClassifier(n_estimators=60), X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df697f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9265442404006677, 0.9365609348914858, 0.9065108514190318]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66f520f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3806343906510851, 0.41068447412353926, 0.5125208681135225]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98caba91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9298831385642737, 0.9599332220367279, 0.9232053422370617]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76527f99",
   "metadata": {},
   "source": [
    "##### Here, we see that the scores from Logistic Regression, Random Forest are giving us reliable and comparable values while SVM is giving us relatively lower scores. So, we can conclude from here that we need a much more reliable model for getting scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d823d11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93611111, 0.87777778, 0.93871866, 0.94986072, 0.87743733])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lr, digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b7798aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96111111, 0.94444444, 0.98328691, 0.98885794, 0.93871866])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(SVC(), digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc57f264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93611111, 0.91666667, 0.95543175, 0.96657382, 0.91643454])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(RandomForestClassifier(n_estimators=60), digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "973f346e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93333333, 0.91666667, 0.95543175, 0.95543175, 0.9275766 ])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(RandomForestClassifier(n_estimators=40), digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b672b66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "381947b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install \"tensorflow>=2.0.0\"\n",
    "# !pip install --upgrade tensorflow-hub\n",
    "# !pip install --upgrade tensorflow_text\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f140b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
